# Fixed Logstash Configuration for ELK Stack
# Enhanced pipeline for processing application logs and sending to Elasticsearch

input {
  # HTTP input for direct log ingestion
  http {
    host => "0.0.0.0"
    port => 8080
    codec => json
  }
  
  # Beats input for log shippers
  beats {
    port => 5044
    host => "0.0.0.0"
  }
  
  # TCP input for structured logs
  tcp {
    port => 5000
    codec => json_lines
    host => "0.0.0.0"
  }
  
  # Syslog input
  syslog {
    port => 514
    host => "0.0.0.0"
  }

  # File input for container logs (if mounted)
  file {
    path => "/var/log/containers/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
    codec => json
  }
}

filter {
  # Parse JSON logs
  if [message] {
    json {
      source => "message"
      target => "parsed"
      on_error => "_jsonparsefailure"
    }
  }
  
  # Add common fields
  mutate {
    add_field => {
      "[@metadata][index_prefix]" => "devsecops-logs"
      "environment" => "%{[parsed][environment]}"
      "service_name" => "%{[parsed][service_name]}"
      "log_source" => "application"
    }
  }
  
  # Handle timestamp parsing
  if [parsed][timestamp] {
    date {
      match => [ "[parsed][timestamp]", "ISO8601", "yyyy-MM-dd HH:mm:ss,SSS", "yyyy-MM-dd HH:mm:ss.SSS" ]
      target => "@timestamp"
    }
  } else if [@timestamp] {
    # Use existing timestamp
    mutate {
      add_field => { "timestamp_source" => "logstash" }
    }
  }
  
  # Extract trace information for correlation
  if [parsed][trace_id] {
    mutate {
      add_field => { "trace_id" => "%{[parsed][trace_id]}" }
    }
  }
  
  if [parsed][span_id] {
    mutate {
      add_field => { "span_id" => "%{[parsed][span_id]}" }
    }
  }
  
  # Categorize log level
  if [parsed][levelname] {
    mutate {
      add_field => { "log_level" => "%{[parsed][levelname]}" }
      lowercase => [ "log_level" ]
    }
  } else if [parsed][level] {
    mutate {
      add_field => { "log_level" => "%{[parsed][level]}" }
      lowercase => [ "log_level" ]
    }
  }
  
  # Extract HTTP request information
  if [parsed][method] {
    mutate {
      add_field => {
        "http_method" => "%{[parsed][method]}"
        "http_path" => "%{[parsed][path]}"
        "http_status" => "%{[parsed][status_code]}"
        "response_time" => "%{[parsed][duration_ms]}"
      }
    }
    
    # Convert response_time to float
    if [response_time] and [response_time] != "null" {
      mutate {
        convert => { "response_time" => "float" }
      }
    }
    
    # Convert http_status to integer
    if [http_status] and [http_status] != "null" {
      mutate {
        convert => { "http_status" => "integer" }
      }
    }
  }
  
  # Extract user information
  if [parsed][user_id] {
    mutate {
      add_field => { "user_id" => "%{[parsed][user_id]}" }
    }
  }
  
  # Extract error information
  if [parsed][error] or [parsed][exception] {
    mutate {
      add_field => { 
        "error_message" => "%{[parsed][error]}"
        "exception_type" => "%{[parsed][exception]}"
        "has_error" => "true"
      }
    }
  } else {
    mutate {
      add_field => { "has_error" => "false" }
    }
  }
  
  # GeoIP enrichment for IP addresses
  if [parsed][remote_addr] and [parsed][remote_addr] != "127.0.0.1" and [parsed][remote_addr] != "localhost" {
    geoip {
      source => "[parsed][remote_addr]"
      target => "geoip"
    }
  }
  
  # User agent parsing
  if [parsed][user_agent] {
    useragent {
      source => "[parsed][user_agent]"
      target => "user_agent"
    }
  }
  
  # Add container information if available
  if [docker][container][name] {
    mutate {
      add_field => { "container_name" => "%{[docker][container][name]}" }
    }
  }
  
  # Clean up and rename fields
  if [service_name] == "%{[parsed][service_name]}" {
    mutate {
      remove_field => [ "service_name" ]
      add_field => { "service_name" => "unknown" }
    }
  }
  
  if [environment] == "%{[parsed][environment]}" {
    mutate {
      remove_field => [ "environment" ]
      add_field => { "environment" => "development" }
    }
  }
  
  # Remove unnecessary fields to reduce storage
  mutate {
    remove_field => [ "message", "host", "@version", "[parsed][timestamp]" ]
  }
}

output {
  # Output to Elasticsearch with proper index management
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"
    template_name => "devsecops-logs"
    template_pattern => "devsecops-logs-*"
    template_overwrite => true
    template => {
      "index_patterns" => ["devsecops-logs-*"],
      "settings" => {
        "number_of_shards" => 1,
        "number_of_replicas" => 0,
        "index.refresh_interval" => "5s",
        "index.mapping.total_fields.limit" => 2000
      },
      "mappings" => {
        "properties" => {
          "@timestamp" => { "type" => "date" },
          "trace_id" => { "type" => "keyword" },
          "span_id" => { "type" => "keyword" },
          "log_level" => { "type" => "keyword" },
          "service_name" => { "type" => "keyword" },
          "environment" => { "type" => "keyword" },
          "container_name" => { "type" => "keyword" },
          "http_method" => { "type" => "keyword" },
          "http_path" => { "type" => "keyword" },
          "http_status" => { "type" => "integer" },
          "response_time" => { "type" => "float" },
          "user_id" => { "type" => "keyword" },
          "error_message" => { "type" => "text" },
          "exception_type" => { "type" => "keyword" },
          "has_error" => { "type" => "boolean" },
          "parsed" => { 
            "type" => "object",
            "enabled" => true
          },
          "geoip" => {
            "properties" => {
              "ip" => { "type" => "ip" },
              "country_name" => { "type" => "keyword" },
              "city_name" => { "type" => "keyword" },
              "location" => { "type" => "geo_point" }
            }
          }
        }
      }
    }
    
    # Connection settings for reliability
    sniffing => false
    manage_template => true
    retry_on_conflict => 3
    
    # Health check settings
    validate_after_inactivity => 200
    http_compression => true
  }
  
  # Debug output for development (remove in production)
  # stdout {
  #   codec => rubydebug
  # }
  
  # Dead letter queue for failed documents
  if "_jsonparsefailure" in [tags] {
    file {
      path => "/usr/share/logstash/data/failed_logs.log"
      codec => json_lines
    }
  }
}
